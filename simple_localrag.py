#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è LocalRAG
–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import os
import json
import time
import hashlib
from typing import List, Dict, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
import asyncio

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class IngestRequest(BaseModel):
    paths: List[str]
    delete_missing: bool = False

class AskRequest(BaseModel):
    question: str

class FeedbackRequest(BaseModel):
    question: str
    llm_answer: str
    citations_used: List[str]
    rating: str
    reason: str
    comment: str
    session_id: str
    request_id: str

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(title="LocalRAG - Real Implementation", version="2.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏
DOCUMENT_STORE = {}
CHUNK_STORE = {}

# –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
def parse_document(file_path: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    content = ""
    file_ext = Path(file_path).suffix.lower()
    
    if file_ext in ['.md', '.txt', '.html', '.htm', '.json', '.csv', '.log']:
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    elif file_ext == '.pdf':
        # PDF —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É
        raise ValueError("PDF format requires additional libraries. Please convert to text format.")
    elif file_ext in ['.docx', '.doc']:
        # Word –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É
        raise ValueError("Word format requires additional libraries. Please convert to text format.")
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è —á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            raise ValueError(f"Unsupported binary format: {file_ext}. Please use text-based formats.")
    
    # –°–æ–∑–¥–∞–µ–º —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    content_hash = hashlib.md5(content.encode()).hexdigest()
    
    return {
        "path": file_path,
        "content": content,
        "content_hash": content_hash,
        "size": len(content),
        "extension": file_ext
    }

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º"""
    chunks = []
    
    # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –∏ –∫—Ä—É–ø–Ω—ã–º —Å–µ–∫—Ü–∏—è–º
    sections = []
    lines = text.split('\n')
    current_section = []
    
    for line in lines:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ —Å #, ** –∏–ª–∏ ==)
        if (line.strip().startswith('#') or 
            line.strip().startswith('**') and line.strip().endswith('**') or
            '==' in line or line.strip().isupper() and len(line.strip()) < 50):
            
            if current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            current_section.append(line)
        else:
            current_section.append(line)
    
    if current_section:
        sections.append('\n'.join(current_section))
    
    # –¢–µ–ø–µ—Ä—å —Ä–∞–∑–±–∏–≤–∞–µ–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º, –∞ –Ω–µ —Å–ª–æ–≤–∞–º
    for section in sections:
        if len(section) <= chunk_size:
            if section.strip():  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                chunks.append(section)
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–µ–∫—Ü–∏–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º
            paragraphs = section.split('\n\n')
            current_chunk = ""
            
            for paragraph in paragraphs:
                if len(current_chunk + '\n\n' + paragraph) <= chunk_size:
                    if current_chunk:
                        current_chunk += '\n\n' + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk)
                    current_chunk = paragraph
                    
                    # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                    if len(current_chunk) > chunk_size:
                        sentences = current_chunk.split('. ')
                        temp_chunk = ""
                        
                        for sentence in sentences:
                            if len(temp_chunk + '. ' + sentence) <= chunk_size:
                                if temp_chunk:
                                    temp_chunk += '. ' + sentence
                                else:
                                    temp_chunk = sentence
                            else:
                                if temp_chunk.strip():
                                    chunks.append(temp_chunk)
                                temp_chunk = sentence
                        
                        if temp_chunk.strip():
                            current_chunk = temp_chunk
            
            if current_chunk.strip():
                chunks.append(current_chunk)
    
    return [chunk for chunk in chunks if chunk.strip()]

def simple_search(query: str, chunks: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å TF-IDF –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    import math
    import re
    
    query_lower = query.lower()
    query_words = set(re.findall(r'\b\w+\b', query_lower))
    results = []
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    synonyms = {
        '–ø–æ–¥–¥–µ—Ä–∂–∫–∞': ['support', '–ø–æ–º–æ—â—å', '—Å–µ—Ä–≤–∏—Å', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–∫–ª–∏–µ–Ω—Ç', '—Å–ª—É–∂–±–∞', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'],
        '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['security', '–∑–∞—â–∏—Ç–∞', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–∫–æ–Ω—Ç—Ä–æ–ª—å', '–±–µ–∑–æ–ø–∞—Å–Ω—ã–π', '–∑–∞—â–∏—â–µ–Ω–Ω—ã–π'],
        '–ø—Ä–æ–¥—É–∫—Ç': ['product', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–µ—Ä–≤–∏—Å', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '—Ä–µ—à–µ–Ω–∏–µ', '—Å–∏—Å—Ç–µ–º–∞', '—Å–æ—Ñ—Ç'],
        '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': ['–¥–∞–Ω–Ω—ã–µ', '—Å–≤–µ–¥–µ–Ω–∏—è', 'details', 'info', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–¥–µ—Ç–∞–ª–∏'],
        '—Ñ—É–Ω–∫—Ü–∏–∏': ['—Ñ—É–Ω–∫—Ü–∏—è', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏', 'features', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–æ–ø—Ü–∏–∏'],
        '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è': ['integration', '–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ', '—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ', '—Å–≤—è–∑—å', 'api'],
        '–≤—Ä–µ–º—è': ['–≤—Ä–µ–º—è', '—á–∞—Å—ã', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≥—Ä–∞—Ñ–∏–∫', 'schedule', 'working'],
        '–∫–æ–Ω—Ç–∞–∫—Ç—ã': ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Å–≤—è–∑—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '–∞–¥—Ä–µ—Å', 'contact']
    }
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    expanded_query_words = set(query_words)
    for word in query_words:
        for key, syns in synonyms.items():
            if word == key or word in syns:
                expanded_query_words.update(syns)
                expanded_query_words.add(key)
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è TF-IDF
    total_docs = len(chunks)
    
    for chunk_id, chunk_data in chunks.items():
        chunk_text = chunk_data['text'].lower()
        chunk_words = re.findall(r'\b\w+\b', chunk_text)
        chunk_word_set = set(chunk_words)
        chunk_word_count = len(chunk_words)
        
        # TF-IDF —Ä–∞—Å—á–µ—Ç—ã
        tf_idf_score = 0
        for word in query_words:
            if word in chunk_words:
                # Term Frequency
                tf = chunk_words.count(word) / chunk_word_count if chunk_word_count > 0 else 0
                
                # Document Frequency (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç)
                docs_containing_word = sum(1 for _, data in chunks.items() 
                                         if word in data['text'].lower())
                
                # Inverse Document Frequency
                idf = math.log(total_docs / max(1, docs_containing_word))
                
                tf_idf_score += tf * idf
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å–ª–æ–≤
        direct_intersection = len(query_words & chunk_word_set)
        expanded_intersection = len(expanded_query_words & chunk_word_set)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑
        phrase_matches = sum(1 for word in query_words if word in chunk_text)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        semantic_score = 0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤–æ–ø—Ä–æ—Å–æ–≤
        categories = {
            'support': ['–ø–æ–¥–¥–µ—Ä–∂–∫', 'support', '–ø–æ–º–æ—â', '–∫–ª–∏–µ–Ω—Ç', '—Å–µ—Ä–≤–∏—Å', '—Å–ª—É–∂–±'],
            'security': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', 'security', '–∑–∞—â–∏—Ç', '–ø–æ–ª–∏—Ç–∏–∫', '–∫–æ–Ω—Ç—Ä–æ–ª'],
            'product': ['–ø—Ä–æ–¥—É–∫—Ç', 'product', '–ø—Ä–∏–ª–æ–∂–µ–Ω', '–ø–ª–∞—Ç—Ñ–æ—Ä–º', '—Å–∏—Å—Ç–µ–º', '—Ä–µ—à–µ–Ω'],
            'features': ['—Ñ—É–Ω–∫—Ü–∏', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç', 'features', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª', '–æ–ø—Ü–∏'],
            'integration': ['–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏', 'integration', '–ø–æ–¥–∫–ª—é—á–µ–Ω', 'api', '—Å–≤—è–∑'],
            'contact': ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Å–≤—è–∑', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '–∞–¥—Ä–µ—Å']
        }
        
        for category, keywords in categories.items():
            query_has_category = any(kw in query_lower for kw in keywords)
            chunk_has_category = any(kw in chunk_text for kw in keywords)
            
            if query_has_category and chunk_has_category:
                semantic_score += 0.8
        
        # –ò—Ç–æ–≥–æ–≤—ã–π score —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        if (direct_intersection > 0 or expanded_intersection > 0 or 
            phrase_matches > 0 or semantic_score > 0 or tf_idf_score > 0):
            
            total_score = (
                tf_idf_score * 0.3 +
                (direct_intersection / max(1, len(query_words))) * 0.25 +
                (expanded_intersection / max(1, len(expanded_query_words))) * 0.2 +
                (phrase_matches / max(1, len(query_words))) * 0.15 +
                semantic_score * 0.1
            )
            
            results.append({
                "chunk_id": chunk_id,
                "text": chunk_data['text'],
                "source": chunk_data['source'],
                "score": round(total_score, 3),
                "metadata": chunk_data.get('metadata', {}),
                "debug": {
                    "tf_idf_score": round(tf_idf_score, 3),
                    "direct_matches": direct_intersection,
                    "expanded_matches": expanded_intersection,
                    "phrase_matches": phrase_matches,
                    "semantic_score": round(semantic_score, 3)
                }
            })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    results.sort(key=lambda x: x['score'], reverse=True)
    return results[:top_k]

def generate_answer_with_ollama(question: str, context_chunks: List[Dict]) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–ª–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    import re
    
    if not context_chunks:
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    question_lower = question.lower()
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
    question_categories = {
        'product': ['–ø—Ä–æ–¥—É–∫—Ç', '—á—Ç–æ', '–æ–ø–∏—Å–∞–Ω–∏–µ', '—Å–∏—Å—Ç–µ–º–∞', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ'],
        'support': ['–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–ø–æ–º–æ—â—å', '–∫–ª–∏–µ–Ω—Ç', '—Å–µ—Ä–≤–∏—Å', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞'],
        'contact': ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Å–≤—è–∑—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '–∞–¥—Ä–µ—Å', '–≤—Ä–µ–º—è'],
        'features': ['—Ñ—É–Ω–∫—Ü–∏', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç', '—É–º–µ–µ—Ç', '–º–æ–∂–µ—Ç', '—Ñ–∏—á–∏'],
        'integration': ['–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ', 'api', '—Å–≤—è–∑–∞—Ç—å'],
        'security': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–∑–∞—â–∏—Ç–∞', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–ø—Ä–∞–≤–∞']
    }
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤–æ–ø—Ä–æ—Å–∞
    main_category = None
    for category, keywords in question_categories.items():
        if any(keyword in question_lower for keyword in keywords):
            main_category = category
            break
    
    relevant_sections = []
    sources = set()
    
    for chunk in context_chunks:
        sources.add(chunk['source'])
        text = chunk['text']
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        sections = extract_relevant_sections(text, question_lower, main_category)
        relevant_sections.extend(sections)
    
    if relevant_sections:
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        unique_sections = list(dict.fromkeys(relevant_sections))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫
        
        answer_parts = []
        for i, section in enumerate(unique_sections[:3]):  # –ë–µ—Ä–µ–º —Ç–æ–ø 3 —Å–µ–∫—Ü–∏–∏
            if section.strip():
                answer_parts.append(f"{section.strip()}")
        
        if answer_parts:
            answer = "\n\n".join(answer_parts)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if sources:
                source_list = ", ".join(sorted(sources))
                answer += f"\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {source_list}"
            
            return answer


def extract_relevant_sections(text: str, question: str, category: str = None) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞"""
    import re
    
    sections = []
    lines = text.split('\n')
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    search_keywords = set()
    
    if category == 'product':
        search_keywords.update(['–ø—Ä–æ–¥—É–∫—Ç', '–æ–ø–∏—Å–∞–Ω–∏–µ', '—Å–∏—Å—Ç–µ–º–∞', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—á—Ç–æ'])
    elif category == 'support':
        search_keywords.update(['–ø–æ–¥–¥–µ—Ä–∂–∫–∞', '–ø–æ–º–æ—â—å', '–∫–ª–∏–µ–Ω—Ç', '—Å–µ—Ä–≤–∏—Å', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞'])
    elif category == 'contact':
        search_keywords.update(['–∫–æ–Ω—Ç–∞–∫—Ç', '—Å–≤—è–∑—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '–≤—Ä–µ–º—è', '—á–∞—Å—ã'])
    elif category == 'features':
        search_keywords.update(['—Ñ—É–Ω–∫—Ü–∏', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç', '–º–æ–∂–µ—Ç', '—É–º–µ–µ—Ç'])
    elif category == 'integration':
        search_keywords.update(['–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏', '–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ', 'api'])
    elif category == 'security':
        search_keywords.update(['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–∑–∞—â–∏—Ç–∞', '–ø–æ–ª–∏—Ç–∏–∫–∞'])
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    question_words = re.findall(r'\b\w+\b', question.lower())
    search_keywords.update(question_words)
    
    i = 0
    while i < len(lines):
        line = lines[i].lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(keyword in line for keyword in search_keywords):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–µ–∫—Ü–∏–∏ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
            if (lines[i].strip().startswith('#') or 
                lines[i].strip().startswith('**') or
                lines[i].strip().isupper() or 
                '==' in lines[i]):
                
                # –≠—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ - –±–µ—Ä–µ–º –µ–≥–æ –∏ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                section_lines = [lines[i]]
                j = i + 1
                
                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                while j < len(lines):
                    next_line = lines[j]
                    if (next_line.strip().startswith('#') or 
                        next_line.strip().startswith('**') or
                        '==' in next_line or
                        (next_line.strip().isupper() and len(next_line.strip()) < 50)):
                        break
                    
                    if next_line.strip():  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        section_lines.append(next_line)
                    j += 1
                
                section = '\n'.join(section_lines)
                if len(section.strip()) > 20:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–∫—Ü–∏–∏
                    sections.append(section)
                
                i = j
            else:
                # –≠—Ç–æ –æ–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º - –±–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥
                start_idx = max(0, i - 2)
                end_idx = min(len(lines), i + 8)
                
                context_lines = []
                for k in range(start_idx, end_idx):
                    if lines[k].strip():
                        context_lines.append(lines[k])
                
                if context_lines:
                    section = '\n'.join(context_lines)
                    if len(section.strip()) > 20:
                        sections.append(section)
                
                i += 1
        else:
            i += 1
    
    return sections

# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
    ollama_status = "connected"
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            ollama_status = "disconnected"
    except:
        ollama_status = "disconnected"
    
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "services": {
            "api": "running",
            "document_store": f"{len(DOCUMENT_STORE)} documents",
            "chunk_store": f"{len(CHUNK_STORE)} chunks",
            "ollama": ollama_status
        },
        "implementation": "Simple LocalRAG with real functionality"
    }

@app.post("/ingest")
async def ingest_documents(request: IngestRequest):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    
    indexed_count = 0
    skipped_count = 0
    errors = []
    
    for file_path in request.paths:
        try:
            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç
            doc_data = parse_document(file_path)
            doc_id = doc_data["content_hash"]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ
            if doc_id in DOCUMENT_STORE:
                skipped_count += 1
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            DOCUMENT_STORE[doc_id] = doc_data
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = chunk_text(doc_data["content"])
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            for i, chunk_content in enumerate(chunks):
                chunk_id = f"{doc_id}_{i:03d}"
                CHUNK_STORE[chunk_id] = {
                    "text": chunk_content,
                    "source": os.path.basename(file_path),
                    "doc_id": doc_id,
                    "chunk_index": i,
                    "metadata": {
                        "path": file_path,
                        "extension": doc_data["extension"]
                    }
                }
                indexed_count += 1
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
            
        except Exception as e:
            errors.append({
                "path": file_path,
                "error": str(e),
                "code": "PROCESSING_ERROR"
            })
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
    
    return {
        "indexed": indexed_count,
        "skipped": skipped_count,
        "errors": errors,
        "doc_id": f"batch_{int(time.time())}"
    }

@app.post("/ask")
async def ask_question(request: AskRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    
    if not CHUNK_STORE:
        raise HTTPException(
            status_code=400, 
            detail="No documents loaded. Please ingest documents first using /ingest endpoint."
        )
    
    start_time = time.time()
    
    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    search_start = time.time()
    search_results = simple_search(request.question, CHUNK_STORE, top_k=5)
    search_time = int((time.time() - search_start) * 1000)
    
    if not search_results:
        return {
            "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.",
            "citations": [],
            "debug": {
                "trace_id": f"trace_{int(time.time())}",
                "search_time_ms": search_time,
                "generation_time_ms": 0,
                "found_chunks": 0
            }
        }
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    gen_start = time.time()
    answer = generate_answer_with_ollama(request.question, search_results)
    gen_time = int((time.time() - gen_start) * 1000)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–∏—Ç–∞—Ç—ã
    citations = []
    for result in search_results:
        citations.append({
            "source": result["source"],
            "chunk_id": result["chunk_id"],
            "relevance_score": round(result["score"], 3),
            "text_preview": result["text"][:200] + "..." if len(result["text"]) > 200 else result["text"]
        })
    
    total_time = int((time.time() - start_time) * 1000)
    
    return {
        "answer": answer,
        "citations": citations,
        "debug": {
            "trace_id": f"trace_{int(time.time())}",
            "search_time_ms": search_time,
            "generation_time_ms": gen_time,
            "total_time_ms": total_time,
            "found_chunks": len(search_results),
            "query_terms": len(request.question.split())
        }
    }

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    feedback_id = f"feedback_{int(time.time())}"
    
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    feedback_data = {
        "feedback_id": feedback_id,
        "timestamp": time.time(),
        "question": request.question,
        "llm_answer": request.llm_answer,
        "rating": request.rating,
        "reason": request.reason,
        "comment": request.comment,
        "session_id": request.session_id,
        "request_id": request.request_id
    }
    
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å
    print(f"üìù –ü–æ–ª—É—á–µ–Ω —Ñ–∏–¥–±–µ–∫: {request.rating} - {request.reason}")
    print(f"   –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {request.comment}")
    
    return {
        "status": "ok",
        "feedback_id": feedback_id,
        "message": "Thank you for your feedback!"
    }

@app.get("/documents")
async def list_documents():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    documents = []
    for doc_id, doc_data in DOCUMENT_STORE.items():
        chunks_count = len([c for c in CHUNK_STORE.values() if c["doc_id"] == doc_id])
        documents.append({
            "doc_id": doc_id,
            "doc_id_short": doc_id[:8] + "...",
            "path": doc_data["path"],
            "filename": os.path.basename(doc_data["path"]),
            "size": doc_data["size"],
            "chunks": chunks_count,
            "extension": doc_data["extension"]
        })
    
    return {
        "total_documents": len(documents),
        "total_chunks": len(CHUNK_STORE),
        "documents": documents
    }

@app.delete("/documents/{doc_id}")
async def delete_document(doc_id: str):
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∏–º —á–∞–Ω–∫–∏"""
    
    if doc_id not in DOCUMENT_STORE:
        raise HTTPException(status_code=404, detail=f"Document with ID {doc_id} not found")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
    doc_info = DOCUMENT_STORE[doc_id]
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    chunks_to_delete = [chunk_id for chunk_id, chunk_data in CHUNK_STORE.items() 
                       if chunk_data["doc_id"] == doc_id]
    
    for chunk_id in chunks_to_delete:
        del CHUNK_STORE[chunk_id]
    
    # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
    del DOCUMENT_STORE[doc_id]
    
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {doc_info['path']} ({len(chunks_to_delete)} —á–∞–Ω–∫–æ–≤)")
    
    return {
        "status": "success",
        "message": f"Document {os.path.basename(doc_info['path'])} deleted successfully",
        "deleted_chunks": len(chunks_to_delete),
        "remaining_documents": len(DOCUMENT_STORE),
        "remaining_chunks": len(CHUNK_STORE)
    }

@app.get("/stats")
async def get_statistics():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "documents_loaded": len(DOCUMENT_STORE),
        "chunks_indexed": len(CHUNK_STORE),
        "document_details": [
            {
                "doc_id": doc_id[:8] + "...",
                "path": doc_data["path"],
                "size": doc_data["size"],
                "chunks": len([c for c in CHUNK_STORE.values() if c["doc_id"] == doc_id])
            }
            for doc_id, doc_data in DOCUMENT_STORE.items()
        ]
    }

if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ LocalRAG")
    print("üìö –≠—Ç–æ—Ç API –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    print("üîç –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ Ollama")
    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:")
    print("  ‚Ä¢ GET /health - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
    print("  ‚Ä¢ POST /ingest - –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("  ‚Ä¢ POST /ask - –≤–æ–ø—Ä–æ—Å—ã –∫ —Å–∏—Å—Ç–µ–º–µ")
    print("  ‚Ä¢ POST /feedback - –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å")
    print("  ‚Ä¢ GET /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã")
    print("\nüåê API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:8000")
    print("üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)