#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è LocalRAG
–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import hashlib
import os
import time
from pathlib import Path
from typing import Any

import requests
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel


# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class IngestRequest(BaseModel):
    paths: list[str]
    delete_missing: bool = False


class AskRequest(BaseModel):
    question: str


class FeedbackRequest(BaseModel):
    question: str
    llm_answer: str
    citations_used: list[str]
    rating: str
    reason: str
    comment: str
    session_id: str
    request_id: str


# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(title="LocalRAG - Real Implementation", version="2.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏
DOCUMENT_STORE = {}
CHUNK_STORE = {}


# –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
def parse_document(file_path: str) -> dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")

    content = ""
    file_ext = Path(file_path).suffix.lower()

    if file_ext in [".md", ".txt", ".html", ".htm", ".json", ".csv", ".log"]:
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
    elif file_ext == ".pdf":
        # PDF —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É
        raise ValueError("PDF format requires additional libraries. Please convert to text format.")
    elif file_ext in [".docx", ".doc"]:
        # Word –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É
        raise ValueError(
            "Word format requires additional libraries. Please convert to text format."
        )
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è —á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()
        except UnicodeDecodeError:
            raise ValueError(
                f"Unsupported binary format: {file_ext}. Please use text-based formats."
            )

    # –°–æ–∑–¥–∞–µ–º —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    content_hash = hashlib.md5(content.encode()).hexdigest()

    return {
        "path": file_path,
        "content": content,
        "content_hash": content_hash,
        "size": len(content),
        "extension": file_ext,
    }


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º"""
    chunks = []

    # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –∏ –∫—Ä—É–ø–Ω—ã–º —Å–µ–∫—Ü–∏—è–º
    sections = []
    lines = text.split("\n")
    current_section = []

    for line in lines:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ —Å #, ** –∏–ª–∏ ==)
        if (
            line.strip().startswith("#")
            or line.strip().startswith("**")
            and line.strip().endswith("**")
            or "==" in line
            or line.strip().isupper()
            and len(line.strip()) < 50
        ):
            if current_section:
                sections.append("\n".join(current_section))
                current_section = []
            current_section.append(line)
        else:
            current_section.append(line)

    if current_section:
        sections.append("\n".join(current_section))

    # –¢–µ–ø–µ—Ä—å —Ä–∞–∑–±–∏–≤–∞–µ–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º, –∞ –Ω–µ —Å–ª–æ–≤–∞–º
    for section in sections:
        if len(section) <= chunk_size:
            if section.strip():  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                chunks.append(section)
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–µ–∫—Ü–∏–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º
            paragraphs = section.split("\n\n")
            current_chunk = ""

            for paragraph in paragraphs:
                if len(current_chunk + "\n\n" + paragraph) <= chunk_size:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk)
                    current_chunk = paragraph

                    # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                    if len(current_chunk) > chunk_size:
                        sentences = current_chunk.split(". ")
                        temp_chunk = ""

                        for sentence in sentences:
                            if len(temp_chunk + ". " + sentence) <= chunk_size:
                                if temp_chunk:
                                    temp_chunk += ". " + sentence
                                else:
                                    temp_chunk = sentence
                            else:
                                if temp_chunk.strip():
                                    chunks.append(temp_chunk)
                                temp_chunk = sentence

                        if temp_chunk.strip():
                            current_chunk = temp_chunk

            if current_chunk.strip():
                chunks.append(current_chunk)

    return [chunk for chunk in chunks if chunk.strip()]


def simple_search(query: str, chunks: dict[str, Any], top_k: int = 5) -> list[dict[str, Any]]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å TF-IDF –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    import math
    import re

    query_lower = query.lower()
    query_words = set(re.findall(r"\b\w+\b", query_lower))
    results = []

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    synonyms = {
        "–ø–æ–¥–¥–µ—Ä–∂–∫–∞": [
            "support",
            "–ø–æ–º–æ—â—å",
            "—Å–µ—Ä–≤–∏—Å",
            "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
            "–∫–ª–∏–µ–Ω—Ç",
            "—Å–ª—É–∂–±–∞",
            "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ",
        ],
        "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": ["security", "–∑–∞—â–∏—Ç–∞", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–∫–æ–Ω—Ç—Ä–æ–ª—å", "–±–µ–∑–æ–ø–∞—Å–Ω—ã–π", "–∑–∞—â–∏—â–µ–Ω–Ω—ã–π"],
        "–ø—Ä–æ–¥—É–∫—Ç": ["product", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Å–µ—Ä–≤–∏—Å", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞", "—Ä–µ—à–µ–Ω–∏–µ", "—Å–∏—Å—Ç–µ–º–∞", "—Å–æ—Ñ—Ç"],
        "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è": ["–¥–∞–Ω–Ω—ã–µ", "—Å–≤–µ–¥–µ–Ω–∏—è", "details", "info", "–æ–ø–∏—Å–∞–Ω–∏–µ", "–¥–µ—Ç–∞–ª–∏"],
        "—Ñ—É–Ω–∫—Ü–∏–∏": ["—Ñ—É–Ω–∫—Ü–∏—è", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏", "features", "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª", "–æ–ø—Ü–∏–∏"],
        "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è": ["integration", "–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ", "—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ", "—Å–≤—è–∑—å", "api"],
        "–≤—Ä–µ–º—è": ["–≤—Ä–µ–º—è", "—á–∞—Å—ã", "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ", "–≥—Ä–∞—Ñ–∏–∫", "schedule", "working"],
        "–∫–æ–Ω—Ç–∞–∫—Ç—ã": ["–∫–æ–Ω—Ç–∞–∫—Ç", "—Å–≤—è–∑—å", "—Ç–µ–ª–µ—Ñ–æ–Ω", "email", "–∞–¥—Ä–µ—Å", "contact"],
        "–≥–æ–ª–æ—Å–æ–≤—ã–µ": ["voip", "–∑–≤–æ–Ω–∫", "–∑–≤—É–∫", "–∞—É–¥–∏–æ", "–≥–æ–ª–æ—Å", "–∫–∞–Ω–∞–ª", "—Ç–µ–ª–µ—Ñ–æ–Ω"],
        "–∑–≤–æ–Ω–∫–∏": ["voip", "–∑–≤–æ–Ω–∫", "–∑–≤—É–∫", "–∞—É–¥–∏–æ", "–≥–æ–ª–æ—Å", "–∫–∞–Ω–∞–ª", "—Ç–µ–ª–µ—Ñ–æ–Ω"],
        "2fa": ["–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω", "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", "–∞—É—Ç–µ–Ω—Ç–∏—Ñ", "otp", "—Ç–æ–∫–µ–Ω", "sms"],
        "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç": ["ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω", "–±–æ—Ç", "–∞–≤—Ç–æ–º–∞—Ç", "–ø–æ–º–æ—â–Ω–∏–∫"],
    }

    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    expanded_query_words = set(query_words)
    for word in query_words:
        for key, syns in synonyms.items():
            if word == key or word in syns:
                expanded_query_words.update(syns)
                expanded_query_words.add(key)

    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è TF-IDF
    total_docs = len(chunks)

    for chunk_id, chunk_data in chunks.items():
        chunk_text = chunk_data["text"].lower()
        chunk_words = re.findall(r"\b\w+\b", chunk_text)
        chunk_word_set = set(chunk_words)
        chunk_word_count = len(chunk_words)

        # TF-IDF —Ä–∞—Å—á–µ—Ç—ã
        tf_idf_score = 0
        for word in query_words:
            if word in chunk_words:
                # Term Frequency
                tf = chunk_words.count(word) / chunk_word_count if chunk_word_count > 0 else 0

                # Document Frequency (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç)
                docs_containing_word = sum(
                    1 for _, data in chunks.items() if word in data["text"].lower()
                )

                # Inverse Document Frequency
                idf = math.log(total_docs / max(1, docs_containing_word))

                tf_idf_score += tf * idf

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å–ª–æ–≤
        direct_intersection = len(query_words & chunk_word_set)
        expanded_intersection = len(expanded_query_words & chunk_word_set)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑
        phrase_matches = sum(1 for word in query_words if word in chunk_text)

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        semantic_score = 0

        # –î–µ—Ç–µ–∫—Ü–∏—è —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞ (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ vs –ø–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ)
        is_existence_question = any(
            pattern in query_lower
            for pattern in [
                "–µ—Å—Ç—å –ª–∏",
                "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏",
                "–¥–æ—Å—Ç—É–ø–Ω",
                "–∏–º–µ–µ—Ç—Å—è –ª–∏",
                "–º–æ–∂–Ω–æ –ª–∏",
                "–±—É–¥–µ—Ç –ª–∏",
                "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –ª–∏",
                "–ø–æ—è–≤–∏—Ç—Å—è –ª–∏",
            ]
        )

        # Boost –¥–ª—è roadmap —Å–µ–∫—Ü–∏–π –ø—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π
        is_roadmap_chunk = any(
            marker in chunk_text
            for marker in ["q1", "q2", "q3", "q4", "–ø–ª–∞–Ω—ã", "roadmap", "–±—É–¥—É—â", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è"]
        )

        if is_existence_question and is_roadmap_chunk:
            semantic_score += 1.2  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è roadmap –ø—Ä–∏ existence –≤–æ–ø—Ä–æ—Å–∞—Ö

        # === CATEGORY-SPECIFIC BOOSTING ===
        # Contact questions - boost –ø–æ–¥–¥–µ—Ä–∂–∫–∞ sections
        if any(
            word in query_lower
            for word in ["–∫–∞–∫ —Å–≤—è–∑–∞—Ç—å—Å—è", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–∫–æ–Ω—Ç–∞–∫—Ç", "email", "—Ç–µ–ª–µ—Ñ–æ–Ω", "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã"]
        ):
            # More precise matching for contact sections
            if any(
                marker in chunk_text
                for marker in [
                    "üîÅ –ø–æ–¥–¥–µ—Ä–∂–∫–∞",
                    "support@",
                    "email:",
                    "telegram-–±–æ—Ç",
                    "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã",
                    "–ø–Ω‚Äì–ø—Ç",
                    "sla:",
                ]
            ):
                semantic_score += 5.0  # Maximum boost for contact sections
            elif any(marker in chunk_text for marker in ["–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "live chat"]):
                semantic_score += 1.0  # Smaller boost for general mentions

        # 2FA questions - boost –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è sections
        if any(
            word in query_lower for word in ["2fa", "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω", "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"]
        ):
            if any(
                marker in chunk_text
                for marker in [
                    "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è",
                    "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è",
                    "2fa",
                    "sms",
                    "authenticator",
                    "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
                ]
            ):
                semantic_score += 2.0  # Strong boost for auth sections

        # AI Assistant questions - boost roadmap sections
        if any(word in query_lower for word in ["ai", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω"]):
            if any(marker in chunk_text for marker in ["ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω", "q4"]):
                semantic_score += 2.0  # Strong boost for AI roadmap

        # VoIP questions - boost roadmap sections
        if any(word in query_lower for word in ["voip", "–≥–æ–ª–æ—Å–æ–≤", "–∑–≤–æ–Ω–∫"]):
            if any(marker in chunk_text for marker in ["voip", "–≥–æ–ª–æ—Å–æ–≤–æ–π –∫–∞–Ω–∞–ª", "q4"]):
                semantic_score += 2.0  # Strong boost for VoIP roadmap

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤–æ–ø—Ä–æ—Å–æ–≤
        categories = {
            "support": ["–ø–æ–¥–¥–µ—Ä–∂–∫", "support", "–ø–æ–º–æ—â", "–∫–ª–∏–µ–Ω—Ç", "—Å–µ—Ä–≤–∏—Å", "—Å–ª—É–∂–±"],
            "security": ["–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç", "security", "–∑–∞—â–∏—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–∫–æ–Ω—Ç—Ä–æ–ª"],
            "product": ["–ø—Ä–æ–¥—É–∫—Ç", "product", "–ø—Ä–∏–ª–æ–∂–µ–Ω", "–ø–ª–∞—Ç—Ñ–æ—Ä–º", "—Å–∏—Å—Ç–µ–º", "—Ä–µ—à–µ–Ω"],
            "features": [
                "—Ñ—É–Ω–∫—Ü–∏",
                "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç",
                "features",
                "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª",
                "–æ–ø—Ü–∏",
                "–≥–æ–ª–æ—Å–æ–≤",
                "voip",
                "–∑–≤–æ–Ω–∫",
            ],
            "integration": ["–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "integration", "–ø–æ–¥–∫–ª—é—á–µ–Ω", "api", "—Å–≤—è–∑"],
            "contact": ["–∫–æ–Ω—Ç–∞–∫—Ç", "—Å–≤—è–∑", "—Ç–µ–ª–µ—Ñ–æ–Ω", "email", "–∞–¥—Ä–µ—Å"],
        }

        for category, keywords in categories.items():
            query_has_category = any(kw in query_lower for kw in keywords)
            chunk_has_category = any(kw in chunk_text for kw in keywords)

            if query_has_category and chunk_has_category:
                semantic_score += 0.8

        # –ò—Ç–æ–≥–æ–≤—ã–π score —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        if (
            direct_intersection > 0
            or expanded_intersection > 0
            or phrase_matches > 0
            or semantic_score > 0
            or tf_idf_score > 0
        ):
            total_score = (
                tf_idf_score * 0.25
                + (direct_intersection / max(1, len(query_words))) * 0.2
                + (expanded_intersection / max(1, len(expanded_query_words))) * 0.15
                + (phrase_matches / max(1, len(query_words))) * 0.15
                + semantic_score * 0.25  # Increased weight for category boosting
            )

            results.append(
                {
                    "chunk_id": chunk_id,
                    "text": chunk_data["text"],
                    "source": chunk_data["source"],
                    "score": round(total_score, 3),
                    "metadata": chunk_data.get("metadata", {}),
                    "debug": {
                        "tf_idf_score": round(tf_idf_score, 3),
                        "direct_matches": direct_intersection,
                        "expanded_matches": expanded_intersection,
                        "phrase_matches": phrase_matches,
                        "semantic_score": round(semantic_score, 3),
                        "is_existence_question": is_existence_question,
                        "is_roadmap_chunk": is_roadmap_chunk,
                    },
                }
            )

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    results.sort(key=lambda x: x["score"], reverse=True)
    return results[:top_k]


def semantic_reranker_with_rules(
    chunks: list[dict], question: str, question_type: str
) -> list[dict]:
    """
    Semantic reranker —Å custom rules –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏.
    Returns: reranked chunks —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ scores
    """

    reranked = []
    question_lower = question.lower()

    for chunk in chunks:
        base_score = chunk["score"]
        chunk_text = chunk["text"].lower()

        # Rule 1: Existing vs Roadmap priority (–∫–ª—é—á–µ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ!)
        is_existing_feature = detect_existing_feature(chunk_text, question_lower)
        is_roadmap_chunk = any(
            marker in chunk_text
            for marker in ["q1", "q2", "q3", "q4", "–ø–ª–∞–Ω—ã", "roadmap", "–±—É–¥—É—â", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è"]
        )

        if question_type in ["existence", "feature_inquiry"]:
            if is_existing_feature and is_roadmap_chunk:
                # Existing feature –í–°–ï–ì–î–ê –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ roadmap –¥–ª—è existence –≤–æ–ø—Ä–æ—Å–æ–≤
                base_score = base_score * 2.5 if is_existing_feature else base_score * 0.3
            elif is_existing_feature:
                base_score *= 2.0  # Boost existing features
            elif is_roadmap_chunk:
                base_score *= 1.5  # Moderate boost roadmap —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç existing

        # Rule 2: Category-specific boosting
        category_boost = get_category_boost(chunk_text, question_lower, question_type)
        base_score *= category_boost

        # Rule 3: Contact —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π boost
        if question_type == "contact" and has_contact_info(chunk_text):
            base_score *= 3.0

        # Rule 4: Security/Privacy boost
        if question_type == "security" and has_security_info(chunk_text):
            base_score *= 2.5

        # Rule 5: Pricing boost
        if question_type == "pricing" and has_pricing_info(chunk_text):
            base_score *= 2.5

        chunk_copy = chunk.copy()
        chunk_copy["reranked_score"] = base_score
        chunk_copy["original_score"] = chunk["score"]
        reranked.append(chunk_copy)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤–æ–º—É score
    return sorted(reranked, key=lambda x: x["reranked_score"], reverse=True)


def detect_existing_feature(text: str, question: str) -> bool:
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç false positive roadmap –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.
    """
    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ HelpZen
    specific_features = {
        "slack": ["slack", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏", "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è:", "zendesk", "salesforce"],
        "live chat": ["live chat", "–≤–∏–¥–∂–µ—Ç", "—á–∞—Ç", "—Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"],
        "2fa": ["–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è", "2fa", "sms", "authy", "google authenticator"],
        "–ø—Ä–æ–±–Ω—ã–π –ø–µ—Ä–∏–æ–¥": ["–ø—Ä–æ–±–Ω—ã–π –ø–µ—Ä–∏–æ–¥", "14 –¥–Ω–µ–π", "—Ç–µ—Å—Ç–æ–≤—ã–π", "trial"],
        "ticketing": ["ticketing system", "—Ç–∏–∫–µ—Ç", "–æ–±—Ä–∞—â–µ–Ω–∏", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"],
        "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞": ["–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "–æ—Ç—á—ë—Ç—ã", "sla"],
        "–ø–æ–¥–¥–µ—Ä–∂–∫–∞": ["support@", "email:", "telegram", "live chat", "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã"],
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏ —Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
    question_lower = question.lower()
    text_lower = text.lower()
    
    for feature, keywords in specific_features.items():
        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        if any(keyword in question_lower for keyword in keywords):
            # –ò —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ (–Ω–æ –ù–ï roadmap)
            if any(keyword in text_lower for keyword in keywords):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ù–ï roadmap —Å–µ–∫—Ü–∏—è
                roadmap_markers = ["q1", "q2", "q3", "q4", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è", "–±—É–¥—É—â–µ–º", "roadmap"]
                is_roadmap = any(marker in text_lower for marker in roadmap_markers)
                if not is_roadmap:
                    return True
    
    # –û–±—â–∏–µ –º–∞—Ä–∫–µ—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π
    existing_markers = [
        "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è",
        "–¥–æ—Å—Ç—É–ø–Ω–æ", 
        "–≤–∫–ª—é—á–∞–µ—Ç",
        "—Ñ—É–Ω–∫—Ü–∏–∏ helpzen",
        "–æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏",
        "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏",
        "–º–µ—Ç–æ–¥—ã –≤—Ö–æ–¥–∞",
        "–º–æ–∂–Ω–æ",
        "—É–º–µ–µ—Ç",
        "—Ç–∞—Ä–∏—Ñ—ã",
        "—Å—Ç–æ–∏–º–æ—Å—Ç—å",
        "–µ—Å—Ç—å",
        "–∏–º–µ–µ—Ç—Å—è",
    ]
    roadmap_markers = ["q1", "q2", "q3", "q4", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è", "–±—É–¥—É—â–µ–º", "roadmap", "–¥–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞"]

    has_existing = any(marker in text_lower for marker in existing_markers)
    has_roadmap = any(marker in text_lower for marker in roadmap_markers)

    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏ —Ç–æ –∏ —Ç–æ, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç existing (–ù–ï roadmap)
    return has_existing and not has_roadmap


def get_category_boost(text: str, question: str, question_type: str) -> float:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π boost –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (20% —Ç–µ—Å—Ç–æ–≤).
    """
    boosts = {
        "contact": 3.5,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        "security": 2.8,  # "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–¥–∞–Ω–Ω—ã–µ"
        "pricing": 2.5,  # "—Ç–∞—Ä–∏—Ñ", "—Å—Ç–æ–∏–º–æ—Å—Ç—å"
        "feature_inquiry": 2.2,  # —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        "instruction": 2.0,  # "–∫–∞–∫", "–≥–¥–µ" - —É–≤–µ–ª–∏—á–µ–Ω
        "existence": 1.5,  # existence –≤–æ–ø—Ä–æ—Å—ã
    }

    text_lower = text.lower()
    question_lower = question.lower()

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ high-priority case-—ã
    if question_type == "contact":
        contact_markers = [
            "support@", "email:", "telegram", "live chat", "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã",
            "–ø–Ω‚Äì–ø—Ç", "sla:", "–æ—Ç–≤–µ—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "—Å–≤—è–∑–∞—Ç—å—Å—è"
        ]
        if any(marker in text_lower for marker in contact_markers):
            return boosts["contact"]
            
    elif question_type == "security":
        security_markers = [
            "aws", "—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ", "soc 2", "–¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è", "aes-", "tls",
            "–ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "—Ñ—Ä–∞–Ω–∫—Ñ—É—Ä—Ç", "–±—ç–∫–∞–ø"
        ]
        if any(marker in text_lower for marker in security_markers):
            return boosts["security"]
            
    elif question_type == "pricing":
        pricing_markers = [
            "$", "—Ç–∞—Ä–∏—Ñ", "–±–µ—Å–ø–ª–∞—Ç–Ω", "–ø–µ—Ä–∏–æ–¥", "pro:", "business:", "free:",
            "/–º–µ—Å", "–∞–≥–µ–Ω—Ç", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü", "–ø—Ä–æ–±–Ω—ã–π"
        ]
        if any(marker in text_lower for marker in pricing_markers):
            return boosts["pricing"]
            
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–æosts –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
    if "2fa" in question_lower or "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω" in question_lower:
        if "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞" in text_lower or "2fa" in text_lower or "authy" in text_lower:
            return 3.5  # –≤—ã—Å–æ–∫–∏–π boost –¥–ª—è 2FA
            
    if "ai" in question_lower or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" in question_lower:
        if "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç" in text_lower or "q4" in text_lower:
            return 3.0  # —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π boost –¥–ª—è AI
            
    if "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã" in question_lower or "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã" in question_lower:
        if "–ø–Ω‚Äì–ø—Ç" in text_lower or "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã" in text_lower:
            return 3.5  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π boost –¥–ª—è —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã

    return boosts.get(question_type, 1.0)


def has_contact_info(text: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    contact_markers = [
        "support@", "email:", "@", "—Ç–µ–ª–µ—Ñ–æ–Ω", "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã", "telegram", "live chat",
        "–ø–Ω‚Äì–ø—Ç", "sla:", "–æ—Ç–≤–µ—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ", "—Å–≤—è–∑–∞—Ç—å—Å—è", "–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è", 
        "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "—Å–ª—É–∂–±–∞", "–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã", "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã"
    ]
    text_lower = text.lower()
    return any(marker in text_lower for marker in contact_markers)


def has_security_info(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    security_markers = ["aws", "—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ", "aes-", "tls", "soc 2", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–ø–æ–ª–∏—Ç–∏–∫"]
    return any(marker in text for marker in security_markers)


def has_pricing_info(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ü–µ–Ω–∞—Ö"""
    pricing_markers = ["$", "—Ç–∞—Ä–∏—Ñ", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–±–µ—Å–ø–ª–∞—Ç–Ω", "–ø—Ä–æ", "business", "–ø–µ—Ä–∏–æ–¥"]
    return any(marker in text for marker in pricing_markers)


def advanced_deduplication(sections: list[str]) -> list[str]:
    """
    –£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å fuzzy matching –∏ line-by-line –∞–Ω–∞–ª–∏–∑–æ–º.
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (35% —Ç–µ—Å—Ç–æ–≤).
    """
    import hashlib
    from difflib import SequenceMatcher

    if not sections:
        return sections

    unique_sections = []
    seen_hashes = set()
    seen_lines = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫

    for section in sections:
        section_clean = section.strip()
        if not section_clean:
            continue

        # –£—Ä–æ–≤–µ–Ω—å 1: –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (MD5 hash)
        content_hash = hashlib.md5(section_clean.encode("utf-8")).hexdigest()
        if content_hash in seen_hashes:
            continue

        # –£—Ä–æ–≤–µ–Ω—å 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è —Å–ø–∏—Å–∫–æ–≤)
        section_lines = [line.strip() for line in section_clean.split("\n") if line.strip()]

        # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% —Å—Ç—Ä–æ–∫ —É–∂–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–µ–∫—Ü–∏—é
        duplicate_lines = sum(1 for line in section_lines if line in seen_lines)
        if section_lines and duplicate_lines / len(section_lines) > 0.5:
            continue

        # –£—Ä–æ–≤–µ–Ω—å 3: Fuzzy matching –¥–ª—è –ø–æ—á—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–µ–∫—Ü–∏–π
        is_similar = False
        for existing in unique_sections:
            # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–∫—Ü–∏–π
            threshold = 0.90 if len(section_clean) < 300 else 0.85

            similarity = SequenceMatcher(
                None, section_clean[:400].lower(), existing[:400].lower()
            ).ratio()

            if similarity > threshold:
                is_similar = True
                break

        if not is_similar:
            unique_sections.append(section_clean)
            seen_hashes.add(content_hash)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ seen_lines
            for line in section_lines:
                if len(line) > 10:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å—Ç—Ä–æ–∫–∏
                    seen_lines.add(line)

    return unique_sections


def enhanced_question_classifier(question_lower: str) -> tuple:
    """
    Enhanced question classification using regex patterns.
    Returns: (question_type, is_existence_question, is_feature_inquiry)
    """
    import re

    # Explicit existence patterns (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ)
    existence_patterns = [
        r"–µ—Å—Ç—å\s+–ª–∏",
        r"–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç\s+–ª–∏",
        r"–¥–æ–±–∞–≤—è—Ç\s+–ª–∏",
        r"–±—É–¥–µ—Ç\s+–ª–∏",
        r"–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è\s+–ª–∏",
        r"–º–æ–∂–Ω–æ\s+–ª–∏",
        r"–∏–º–µ–µ—Ç—Å—è\s+–ª–∏",
        r"–ø–æ—è–≤–∏—Ç—Å—è\s+–ª–∏",
        r"–¥–æ—Å—Ç—É–ø–Ω\w*\s*(–ª–∏)?",
        r"–≤–∫–ª—é—á–∞–µ—Ç\s+–ª–∏",
        r"—É–º–µ–µ—Ç\s+–ª–∏",
        r"–≤–æ–∑–º–æ–∂–Ω\w*\s*(–ª–∏)?",
        r"–ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω\w*\s*(–ª–∏)?",
        r"—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\w*\s*(–ª–∏)?",
        r"—Ä–∞–±–æ—Ç–∞–µ—Ç\s+–ª–∏",
        r"—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç\s+–ª–∏",
        r"–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞\w+\s*(–ª–∏)?",
        r"–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç\s+–ª–∏",
        r"^(–µ—Å—Ç—å|–∏–º–µ–µ—Ç—Å—è|–¥–æ—Å—Ç—É–ø–Ω–∞?)\s+",
        r"–≤—Å—Ç—Ä–æ–µ–Ω\w*\s*(–ª–∏)?",
        r"–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω\w*\s*(–ª–∏)?",
    ]

    # Instruction patterns (–∫–∞–∫, –≥–¥–µ, –∫–æ–≥–¥–∞)
    instruction_patterns = [
        r"–∫–∞–∫\s+\w+",
        r"–≥–¥–µ\s+\w+",
        r"–∫–æ–≥–¥–∞\s+\w+",
        r"–∫–∞–∫–∏–º\s+–æ–±—Ä–∞–∑–æ–º",
        r"—á—Ç–æ\s+–¥–µ–ª–∞—Ç—å",
        r"–∫–∞–∫\s+—Å–≤—è–∑–∞—Ç—å—Å—è",
        r"–∫–∞–∫\s+\w+\s+\w+",
        r"–≥–¥–µ\s+–Ω–∞–π—Ç–∏",
        r"–≥–¥–µ\s+\w+\s+\w+",
    ]

    # Technical feature inquiry (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
    feature_patterns = [
        r"^(ai|–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç|voip|2fa|–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω)\s*$",
        r"^(ai[\-\s]*–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç|–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω\w*\s*–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç)\s*$",
        r"^(whatsapp|telegram|slack)\s*(–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è|api)?\s*$",
        r"–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è\s+—Å\s+\w+",
        r"^(live\s+chat|–ª–∞–π–≤\s+—á–∞—Ç|—á–∞—Ç)\s*$",
        r"^(ticketing|—Ç–∏–∫–µ—Ç–∏–Ω–≥|—Å–∏—Å—Ç–µ–º–∞\s+—Ç–∏–∫–µ—Ç–æ–≤)\s*$",
        r"^(–±–∞–∑–∞\s+–∑–Ω–∞–Ω–∏–π|knowledge\s+base)\s*$",
        r"^(–∞–Ω–∞–ª–∏—Ç–∏–∫–∞|analytics|–æ—Ç—á–µ—Ç—ã)\s*$",
        r"–ø–æ–¥–¥–µ—Ä–∂–∫–∞\s+(whatsapp|telegram|slack|facebook)",
        r"–≥–æ–ª–æ—Å–æ–≤–∞—è\s+–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
        r"api\s+–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
        r"^(–±–∞–∑–∞\s+–∑–Ω–∞–Ω–∏–π|live\s+chat|ticketing)\s*$",
        r"^(–≥–æ–ª–æ—Å–æ–≤|–∑–≤–æ–Ω–∫)\w*\s*(–ø–æ–¥–¥–µ—Ä–∂–∫–∞|–∫–∞–Ω–∞–ª)?\s*$",
    ]

    # Pricing/info patterns
    pricing_patterns = [
        r"—Å–∫–æ–ª—å–∫–æ\s+—Å—Ç–æ–∏—Ç",
        r"—Ü–µ–Ω–∞\s+\w+",
        r"—Ç–∞—Ä–∏—Ñ",
        r"—Å—Ç–æ–∏–º–æ—Å—Ç—å",
        r"–ø—Ä–æ–±–Ω—ã–π\s+–ø–µ—Ä–∏–æ–¥",
    ]

    # Contact patterns (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏)
    contact_patterns = [
        r"–∫–∞–∫\s+—Å–≤—è–∑–∞—Ç—å—Å—è",
        r"–∫–∞–∫\s+–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è",
        r"–∫–æ–Ω—Ç–∞–∫—Ç\w*",
        r"–ø–æ–¥–¥–µ—Ä–∂–∫\w*\s*(—Å–≤—è–∑—å|—Å–ª—É–∂–±–∞)?",
        r"email\s+–ø–æ–¥–¥–µ—Ä–∂–∫–∏",
        r"—á–∞—Å—ã\s+—Ä–∞–±–æ—Ç—ã",
        r"–≤—Ä–µ–º—è\s+—Ä–∞–±–æ—Ç—ã",
        r"–≥—Ä–∞—Ñ–∏–∫\s+—Ä–∞–±–æ—Ç—ã",
        r"—Å–≤—è–∑—å\s+—Å\s+–ø–æ–¥–¥–µ—Ä–∂–∫–æ–π",
        r"—Å–ª—É–∂–±–∞\s+–ø–æ–¥–¥–µ—Ä–∂–∫–∏",
        r"—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
        r"—Ç–µ–ª–µ—Ñ–æ–Ω\s+–ø–æ–¥–¥–µ—Ä–∂–∫–∏",
        r"–Ω–∞–ø–∏—Å–∞—Ç—å\s+–≤\s+–ø–æ–¥–¥–µ—Ä–∂–∫—É",
        r"–æ–±—Ä–∞—â–µ–Ω–∏–µ\s+–≤\s+–ø–æ–¥–¥–µ—Ä–∂–∫—É",
    ]

    # Security patterns
    security_patterns = [
        r"–≥–¥–µ\s+—Ö—Ä–∞–Ω—è—Ç—Å—è\s+–¥–∞–Ω–Ω—ã–µ",
        r"–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        r"—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ",
        r"–ø–æ–ª–∏—Ç–∏–∫–∞\s+–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    ]

    # Check patterns in order of specificity
    is_existence_question = any(
        re.search(pattern, question_lower) for pattern in existence_patterns
    )
    is_instruction = any(re.search(pattern, question_lower) for pattern in instruction_patterns)
    is_feature_inquiry = any(re.search(pattern, question_lower) for pattern in feature_patterns)
    is_pricing = any(re.search(pattern, question_lower) for pattern in pricing_patterns)
    is_contact = any(re.search(pattern, question_lower) for pattern in contact_patterns)
    is_security = any(re.search(pattern, question_lower) for pattern in security_patterns)

    # Determine question type (most specific first)
    if is_existence_question:
        question_type = "existence"
    elif is_feature_inquiry:
        question_type = "feature_inquiry"
    elif is_instruction:
        question_type = "instruction"
    elif is_contact:
        question_type = "contact"
    elif is_pricing:
        question_type = "pricing"
    elif is_security:
        question_type = "security"
    else:
        question_type = "general"

    # For backwards compatibility, also return old flags
    return question_type, is_existence_question, is_feature_inquiry


def generate_answer_with_ollama(question: str, context_chunks: list[dict]) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–ª–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    import re

    if not context_chunks:
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    question_lower = question.lower()

    # Enhanced Question Classification
    question_type, is_existence_question, is_feature_inquiry = enhanced_question_classifier(
        question_lower
    )

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
    question_categories = {
        "product": ["–ø—Ä–æ–¥—É–∫—Ç", "—á—Ç–æ", "–æ–ø–∏—Å–∞–Ω–∏–µ", "—Å–∏—Å—Ç–µ–º–∞", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"],
        "support": ["–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–ø–æ–º–æ—â—å", "–∫–ª–∏–µ–Ω—Ç", "—Å–µ—Ä–≤–∏—Å", "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞"],
        "contact": [
            "–∫–æ–Ω—Ç–∞–∫—Ç",
            "—Å–≤—è–∑—å",
            "—Ç–µ–ª–µ—Ñ–æ–Ω",
            "email",
            "–∞–¥—Ä–µ—Å",
            "–≤—Ä–µ–º—è",
            "—Å–≤—è–∑–∞—Ç—å—Å—è",
            "–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è",
        ],
        "features": ["—Ñ—É–Ω–∫—Ü–∏", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç", "—É–º–µ–µ—Ç", "–º–æ–∂–µ—Ç", "—Ñ–∏—á–∏", "–≥–æ–ª–æ—Å–æ–≤", "voip"],
        "integration": ["–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ", "api", "—Å–≤—è–∑–∞—Ç—å"],
        "security": ["–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–∑–∞—â–∏—Ç–∞", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–ø—Ä–∞–≤–∞"],
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤–æ–ø—Ä–æ—Å–∞
    main_category = None
    for category, keywords in question_categories.items():
        if any(keyword in question_lower for keyword in keywords):
            main_category = category
            break

    relevant_sections = []
    roadmap_sections = []
    sources = set()

    for chunk in context_chunks:
        sources.add(chunk["source"])
        text = chunk["text"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ roadmap —Å–µ–∫—Ü–∏–µ–π
        is_roadmap = any(
            marker in text.lower()
            for marker in ["q1", "q2", "q3", "q4", "–ø–ª–∞–Ω—ã", "roadmap", "–±—É–¥—É—â", "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è"]
        )

        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        sections = extract_relevant_sections(text, question_lower, main_category, is_roadmap)

        if is_roadmap:
            roadmap_sections.extend(sections)
        else:
            relevant_sections.extend(sections)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏ –ø–ª–∞–Ω–∏—Ä—É–µ–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è existence –≤–æ–ø—Ä–æ—Å–æ–≤
    if (is_existence_question or is_feature_inquiry) and roadmap_sections:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏—Å–∫–æ–º–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–µ–∫—Ü–∏—è—Ö
        question_keywords = [
            word
            for word in question_lower.split()
            if word not in ["–µ—Å—Ç—å", "–ª–∏", "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç", "–¥–æ—Å—Ç—É–ø–Ω", "–º–æ–∂–Ω–æ", "–±—É–¥–µ—Ç"]
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≤ —Ç–µ–∫—É—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö
        has_current_feature = False
        if relevant_sections:
            for section in relevant_sections:
                section_lower = section.lower()
                # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
                specific_found = False

                # –î–ª—è WhatsApp - –∏—â–µ–º –∏–º–µ–Ω–Ω–æ WhatsApp, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"
                if "whatsapp" in question_lower:
                    if "whatsapp" in section_lower and not any(
                        q in section_lower for q in ["q1", "q2", "q3", "q4"]
                    ):
                        specific_found = True
                # –î–ª—è VoIP/–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
                elif any(word in question_lower for word in ["voip", "–≥–æ–ª–æ—Å–æ–≤", "–∑–≤–æ–Ω–∫"]):
                    if any(
                        word in section_lower for word in ["voip", "–≥–æ–ª–æ—Å–æ–≤", "–∑–≤–æ–Ω–∫"]
                    ) and not any(q in section_lower for q in ["q1", "q2", "q3", "q4"]):
                        specific_found = True
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π - –æ–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                else:
                    if (
                        any(keyword in section_lower for keyword in question_keywords)
                        and any(
                            marker in section_lower
                            for marker in ["—Ñ—É–Ω–∫—Ü–∏", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç", "–ø–æ–¥–¥–µ—Ä–∂", "–≤–∫–ª—é—á–∞–µ—Ç"]
                        )
                        and not any(q in section_lower for q in ["q1", "q2", "q3", "q4"])
                    ):
                        specific_found = True

                if specific_found:
                    has_current_feature = True
                    break

        # –î–ª—è existence –≤–æ–ø—Ä–æ—Å–æ–≤ –≤—Å–µ–≥–¥–∞ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º roadmap, –µ—Å–ª–∏ –æ–Ω –Ω–∞–π–¥–µ–Ω
        if roadmap_sections:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é –∫ roadmap —Å–µ–∫—Ü–∏—è–º
            unique_roadmap_sections = advanced_deduplication(roadmap_sections)
            
            # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é roadmap —Å–µ–∫—Ü–∏—é –∫ –≤–æ–ø—Ä–æ—Å—É
            best_roadmap = None
            best_score = 0

            for section in unique_roadmap_sections:
                section_lower = section.lower()
                # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å roadmap —Å–µ–∫—Ü–∏–∏ –∫ –≤–æ–ø—Ä–æ—Å—É
                matches = sum(1 for keyword in question_keywords if keyword in section_lower)
                if matches > best_score:
                    best_score = matches
                    best_roadmap = section

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é
            roadmap_info = best_roadmap if best_roadmap else unique_roadmap_sections[0]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–≤–∞—Ä—Ç–∞–ª –∏–∑ roadmap
            quarter_match = re.search(r"q[1-4]", roadmap_info.lower())
            quarter = quarter_match.group().upper() if quarter_match else "–≤ –±—É–¥—É—â–µ–º"

            answer = "‚ùå **–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.**\n\n"
            answer += f"‚úÖ **–ù–æ –æ–Ω–∞ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ {quarter}:**\n\n"
            answer += roadmap_info.strip()

            if sources:
                source_list = ", ".join(sorted(sources))
                answer += f"\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {source_list}"

            return answer

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    all_sections = relevant_sections + roadmap_sections
    if all_sections:
        # Stage 3: Advanced Deduplication —Å fuzzy matching
        unique_sections = advanced_deduplication(all_sections)

        answer_parts = []
        for i, section in enumerate(unique_sections[:3]):  # –ë–µ—Ä–µ–º —Ç–æ–ø 3 —Å–µ–∫—Ü–∏–∏
            if section.strip():
                answer_parts.append(f"{section.strip()}")

        if answer_parts:
            # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            answer = "\n\n".join(answer_parts)
            if sources:
                source_list = ", ".join(sorted(sources))
                answer += f"\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** {source_list}"
            return answer


def parse_markdown_table(text: str) -> dict[str, list[str]]:
    """–ü–∞—Ä—Å–∏—Ç markdown —Ç–∞–±–ª–∏—Ü—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    lines = [line.strip() for line in text.split("\n") if line.strip()]

    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    header_line = None
    separator_line = None
    data_lines = []

    for i, line in enumerate(lines):
        if "|" in line and header_line is None:
            header_line = line
        elif header_line and "---" in line or "===" in line:
            separator_line = line
        elif header_line and separator_line and "|" in line:
            data_lines.append(line)

    if not header_line or not data_lines:
        return {}

    # –ü–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = [h.strip() for h in header_line.split("|") if h.strip()]

    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    table_data = []
    for line in data_lines:
        row = [cell.strip() for cell in line.split("|") if cell.strip()]
        if len(row) >= len(headers):
            table_data.append(dict(zip(headers, row[: len(headers)])))

    return {"headers": headers, "rows": table_data}


def filter_table_by_query(table_data: dict, query: str) -> list[dict]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É"""
    if not table_data or not table_data.get("rows"):
        return []

    query_words = set(query.lower().split())
    relevant_rows = []

    for row in table_data["rows"]:
        # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏
        row_text = " ".join(row.values()).lower()
        matches = sum(1 for word in query_words if word in row_text)

        if matches > 0:
            relevant_rows.append({"row": row, "relevance": matches / len(query_words)})

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    relevant_rows.sort(key=lambda x: x["relevance"], reverse=True)
    return [item["row"] for item in relevant_rows]


def format_table_response(headers: list[str], rows: list[dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤ —á–∏—Ç–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç"""
    if not rows:
        return ""

    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    response = "| " + " | ".join(headers) + " |\n"
    response += "| " + " | ".join(["---"] * len(headers)) + " |\n"

    for row in rows:
        row_values = [row.get(header, "") for header in headers]
        response += "| " + " | ".join(row_values) + " |\n"

    return response


def detect_answer_type(sections: list[str], question: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —à–∞–±–ª–æ–Ω–∞"""
    question_lower = question.lower()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ–∫—Ü–∏–π
    combined_text = " ".join(sections).lower()

    # –ü–†–ò–û–†–ò–¢–ï–¢: schedule –ø—Ä–æ–≤–µ—Ä—è–µ–º –ü–ï–†–í–´–ú (–¥–æ contact!)
    if any(
        pattern in question_lower
        for pattern in ["—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã", "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã", "–∫–∞–∫–∏–µ —á–∞—Å—ã", "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ", "–∫–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç"]
    ):
        return "schedule"
    elif any("|" in section and ("---" in section or "===" in section) for section in sections):
        return "table"
    elif any(pattern in question_lower for pattern in ["—á—Ç–æ", "–∫–∞–∫–æ–π", "–æ–ø–∏—Å–∞–Ω–∏–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"]):
        return "description"
    elif any(
        pattern in question_lower for pattern in ["–∫–∞–∫", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "steps"]
    ):
        return "howto"
    elif any(pattern in question_lower for pattern in ["–∫–æ–Ω—Ç–∞–∫—Ç", "—Å–≤—è–∑—å", "—Ç–µ–ª–µ—Ñ–æ–Ω", "email"]):
        return "contact"
    else:
        return "general"


def format_structured_answer(sections: list[str], question: str, sources: set) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É —à–∞–±–ª–æ–Ω—É"""
    answer_type = detect_answer_type(sections, question)

    if answer_type == "description":
        # –§–æ—Ä–º–∞—Ç: –ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Üí –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí –î–µ—Ç–∞–ª–∏
        answer = ""
        for section in sections[:2]:  # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º 2 —Å–µ–∫—Ü–∏–∏ –¥–ª—è –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            if section.strip():
                # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –∫–∞–∫ –µ—Å—Ç—å
                if section.strip().startswith("#") or section.strip().startswith("**"):
                    answer += f"{section.strip()}\n\n"
                else:
                    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç - –¥–æ–±–∞–≤–ª—è–µ–º —Å –Ω–µ–±–æ–ª—å—à–∏–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                    lines = section.strip().split("\n")
                    if len(lines) > 1 and lines[0]:
                        answer += f"**{lines[0]}**\n\n"
                        answer += "\n".join(lines[1:]) + "\n\n"
                    else:
                        answer += f"{section.strip()}\n\n"

    elif answer_type == "table":
        # –¢–∞–±–ª–∏—Ü—ã —É–∂–µ —Ö–æ—Ä–æ—à–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã
        answer = "\n\n".join(sections)

    elif answer_type == "howto":
        # –§–æ—Ä–º–∞—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        answer = "**–í–æ—Ç –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å:**\n\n"
        answer += "\n\n".join(sections)

    elif answer_type == "schedule":
        # –§–æ—Ä–º–∞—Ç –¥–ª—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π –∏ –≤—Ä–µ–º–µ–Ω–∏ - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –°–¢–†–û–ì–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        answer = "‚è∞ **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã:**\n\n"
        relevant_sections = []

        # –ò—â–µ–º –¢–û–õ–¨–ö–û —Å–µ–∫—Ü–∏–∏ —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
        for section in sections:
            section_lower = section.lower()

            # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —Å–µ–∫—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏
            has_schedule_info = (
                "–ø–Ω‚Äì–ø—Ç" in section_lower
                or "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫" in section_lower
                or ("—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã" in section_lower and ("gmt" in section_lower or ":" in section))
                or (": " in section and ("—á–∞—Å—ã" in section_lower or "–≤—Ä–µ–º—è" in section_lower))
            )

            # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ª—é–±—ã–µ —Å–µ–∫—Ü–∏–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏, –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏, —Ç–∞—Ä–∏—Ñ–∞–º–∏
            has_irrelevant_content = any(
                bad_word in section_lower
                for bad_word in [
                    "live chat",
                    "ticketing",
                    "—Ñ—É–Ω–∫—Ü–∏–∏ helpzen",
                    "–æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏",
                    "email-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
                    "–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π",
                    "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞",
                    "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏",
                    "slack",
                    "telegram",
                    "facebook",
                    "crm",
                    "—Ç–∞—Ä–∏—Ñ—ã",
                    "free:",
                    "pro:",
                    "business:",
                ]
            )

            if has_schedule_info and not has_irrelevant_content:
                relevant_sections.append(section)

        if relevant_sections:
            answer += "\n\n".join(relevant_sections[:1])  # –¢–û–õ–¨–ö–û –ø–µ—Ä–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å–µ–∫—Ü–∏—è
        else:
            # Fallback: –∏—â–µ–º —Ö–æ—Ç—è –±—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —á–∞—Å–æ–≤
            for section in sections:
                if "—á–∞—Å—ã" in section.lower() and len(section) < 200:  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–∫—Ü–∏–∏ —Å —á–∞—Å–∞–º–∏
                    answer += section
                    break

    elif answer_type == "contact":
        # –§–æ—Ä–º–∞—Ç –¥–ª—è –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
        answer = "üìû **–ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**\n\n"
        relevant_sections = []
        for section in sections:
            if any(
                keyword in section.lower()
                for keyword in [
                    "–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
                    "support@",
                    "email:",
                    "telegram-–±–æ—Ç",
                    "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã",
                    "–∫–æ–Ω—Ç–∞–∫—Ç",
                ]
            ):
                relevant_sections.append(section)
        answer += "\n\n".join(relevant_sections[:3])  # –ú–∞–∫—Å–∏–º—É–º 3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏

    else:
        # –û–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç
        answer = "\n\n".join(sections)

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if sources:
        source_list = ", ".join(sorted(sources))
        answer += f"\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** {source_list}"

    return answer.strip()


def extract_relevant_sections(
    text: str, question: str, category: str = None, is_roadmap: bool = False
) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞"""
    import re

    sections = []
    lines = text.split("\n")

    # –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ —á–∞—Å–∞—Ö —Ä–∞–±–æ—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏
    if any(
        keyword in question.lower()
        for keyword in [
            "—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã",
            "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã",
            "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ",
            "–∫–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç",
            "–∫–∞–∫–∏–µ —á–∞—Å—ã",
            "—á–∞—Å—ã",
        ]
    ):
        # –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Å–µ–∫—Ü–∏–∏ —Å —á–∞—Å–∞–º–∏ —Ä–∞–±–æ—Ç—ã
        schedule_section = ""
        found_schedule = False

        for i, line in enumerate(lines):
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å —á–∞—Å–∞–º–∏ —Ä–∞–±–æ—Ç—ã
            if any(keyword in line.lower() for keyword in ["—á–∞—Å—ã —Ä–∞–±–æ—Ç—ã", "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã"]):
                schedule_section += line + "\n"
                found_schedule = True
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
                for j in range(i + 1, min(i + 5, len(lines))):
                    next_line = lines[j]
                    if next_line.strip():
                        if (
                            "–ø–Ω‚Äì–ø—Ç" in next_line.lower()
                            or "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫" in next_line.lower()
                            or ":" in next_line
                            or "gmt" in next_line.lower()
                        ):
                            schedule_section += next_line + "\n"
                        elif next_line.startswith("#") or next_line.startswith("**"):
                            break  # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è
                break
            # –ò–ª–∏ –Ω–∞—Ö–æ–¥–∏–º –ø—Ä—è–º–æ —Å—Ç—Ä–æ–∫—É —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
            elif any(keyword in line.lower() for keyword in ["–ø–Ω‚Äì–ø—Ç", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–≤—Ä–µ–º—è:"]):
                if not found_schedule:
                    schedule_section += "#### –ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã:\n"
                schedule_section += line + "\n"
                found_schedule = True

        if schedule_section.strip():
            return [schedule_section.strip()]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—É
    has_table = any("|" in line and ("---" in text or "===" in text) for line in lines)

    if has_table:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É
        table_data = parse_markdown_table(text)
        if table_data and table_data.get("rows"):
            filtered_rows = filter_table_by_query(table_data, question)
            if filtered_rows:
                formatted_table = format_table_response(table_data["headers"], filtered_rows)
                sections.append(formatted_table)
                return sections

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    search_keywords = set()

    if category == "product":
        search_keywords.update(["–ø—Ä–æ–¥—É–∫—Ç", "–æ–ø–∏—Å–∞–Ω–∏–µ", "—Å–∏—Å—Ç–µ–º–∞", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—á—Ç–æ"])
    elif category == "support":
        search_keywords.update(["–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–ø–æ–º–æ—â—å", "–∫–ª–∏–µ–Ω—Ç", "—Å–µ—Ä–≤–∏—Å", "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞"])
    elif category == "contact":
        search_keywords.update(["–∫–æ–Ω—Ç–∞–∫—Ç", "—Å–≤—è–∑—å", "—Ç–µ–ª–µ—Ñ–æ–Ω", "email", "–≤—Ä–µ–º—è", "—á–∞—Å—ã"])
    elif category == "features":
        search_keywords.update(["—Ñ—É–Ω–∫—Ü–∏", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç", "–º–æ–∂–µ—Ç", "—É–º–µ–µ—Ç"])
    elif category == "integration":
        search_keywords.update(["–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ", "api"])
    elif category == "security":
        search_keywords.update(["–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–∑–∞—â–∏—Ç–∞", "–ø–æ–ª–∏—Ç–∏–∫–∞"])

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    question_words = re.findall(r"\b\w+\b", question.lower())
    search_keywords.update(question_words)

    i = 0
    while i < len(lines):
        line = lines[i].lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(keyword in line for keyword in search_keywords):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–µ–∫—Ü–∏–∏ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
            if (
                lines[i].strip().startswith("#")
                or lines[i].strip().startswith("**")
                or lines[i].strip().isupper()
                or "==" in lines[i]
            ):
                # –≠—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ - –±–µ—Ä–µ–º –µ–≥–æ –∏ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                section_lines = [lines[i]]
                j = i + 1

                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                while j < len(lines):
                    next_line = lines[j]
                    if (
                        next_line.strip().startswith("#")
                        or next_line.strip().startswith("**")
                        or "==" in next_line
                        or (next_line.strip().isupper() and len(next_line.strip()) < 50)
                    ):
                        break

                    if next_line.strip():  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        section_lines.append(next_line)
                    j += 1

                section = "\n".join(section_lines)
                if len(section.strip()) > 20:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–∫—Ü–∏–∏
                    sections.append(section)

                i = j
            else:
                # –≠—Ç–æ –æ–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º - –±–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥
                start_idx = max(0, i - 2)
                end_idx = min(len(lines), i + 8)

                context_lines = []
                for k in range(start_idx, end_idx):
                    if lines[k].strip():
                        context_lines.append(lines[k])

                if context_lines:
                    section = "\n".join(context_lines)
                    if len(section.strip()) > 20:
                        sections.append(section)

                i += 1
        else:
            i += 1

    return sections


def calculate_search_quality_metrics(
    chunks: list[dict], question: str, question_type: str = None
) -> dict[str, Any]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""
    if not chunks:
        return {
            "coverage_score": 0.0,
            "relevance_average": 0.0,
            "relevance_variance": 0.0,
            "chunk_diversity": 0.0,
            "roadmap_coverage": False,
            "table_detection": False,
        }

    # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    scores = [chunk.get("score", 0) for chunk in chunks]
    avg_relevance = sum(scores) / len(scores) if scores else 0

    # –î–∏—Å–ø–µ—Ä—Å–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã score'—ã)
    variance = (
        sum((score - avg_relevance) ** 2 for score in scores) / len(scores)
        if len(scores) > 1
        else 0
    )

    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    sources = set(chunk.get("source", "") for chunk in chunks)
    diversity = len(sources) / len(chunks) if chunks else 0

    # –ü–æ–∫—Ä—ã—Ç–∏–µ roadmap (–¥–ª—è existence –≤–æ–ø—Ä–æ—Å–æ–≤)
    is_existence_q = any(
        pattern in question.lower()
        for pattern in ["–µ—Å—Ç—å –ª–∏", "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏", "–¥–æ—Å—Ç—É–ø–Ω", "–±—É–¥–µ—Ç –ª–∏"]
    )

    # –¢–∞–∫–∂–µ —É—á–∏—Ç—ã–≤–∞–µ–º feature inquiry
    is_feature_q = (
        any(
            feature in question.lower()
            for feature in ["whatsapp", "voip", "–≥–æ–ª–æ—Å–æ–≤", "–∑–≤–æ–Ω–∫", "telegram", "slack"]
        )
        and len(question.lower().split()) <= 5
    )
    roadmap_found = any(chunk.get("debug", {}).get("is_roadmap_chunk", False) for chunk in chunks)

    # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
    table_found = any("|" in chunk.get("text", "") for chunk in chunks)

    # –ü–æ–∫—Ä—ã—Ç–∏–µ (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º—ã –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é)
    coverage = min(1.0, len(chunks) / 3) * avg_relevance  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏ –∫–∞—á–µ—Å—Ç–≤—É

    return {
        "coverage_score": round(coverage, 3),
        "relevance_average": round(avg_relevance, 3),
        "relevance_variance": round(variance, 3),
        "chunk_diversity": round(diversity, 3),
        "roadmap_coverage": roadmap_found if (is_existence_q or is_feature_q) else None,
        "table_detection": table_found,
        "total_chunks": len(chunks),
        "question_type": question_type
        if question_type
        else ("existence" if (is_existence_q or is_feature_q) else "general"),
    }


# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
    ollama_status = "connected"
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            ollama_status = "disconnected"
    except:
        ollama_status = "disconnected"

    return {
        "status": "healthy",
        "timestamp": time.time(),
        "services": {
            "api": "running",
            "document_store": f"{len(DOCUMENT_STORE)} documents",
            "chunk_store": f"{len(CHUNK_STORE)} chunks",
            "ollama": ollama_status,
        },
        "implementation": "Simple LocalRAG with real functionality",
    }


@app.post("/ingest")
async def ingest_documents(request: IngestRequest):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""

    indexed_count = 0
    skipped_count = 0
    errors = []

    for file_path in request.paths:
        try:
            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç
            doc_data = parse_document(file_path)
            doc_id = doc_data["content_hash"]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ
            if doc_id in DOCUMENT_STORE:
                skipped_count += 1
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            DOCUMENT_STORE[doc_id] = doc_data

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = chunk_text(doc_data["content"])

            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            for i, chunk_content in enumerate(chunks):
                chunk_id = f"{doc_id}_{i:03d}"
                CHUNK_STORE[chunk_id] = {
                    "text": chunk_content,
                    "source": os.path.basename(file_path),
                    "doc_id": doc_id,
                    "chunk_index": i,
                    "metadata": {"path": file_path, "extension": doc_data["extension"]},
                }
                indexed_count += 1

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")

        except Exception as e:
            errors.append({"path": file_path, "error": str(e), "code": "PROCESSING_ERROR"})
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")

    return {
        "indexed": indexed_count,
        "skipped": skipped_count,
        "errors": errors,
        "doc_id": f"batch_{int(time.time())}",
    }


@app.post("/ask")
async def ask_question(request: AskRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""

    if not CHUNK_STORE:
        raise HTTPException(
            status_code=400,
            detail="No documents loaded. Please ingest documents first using /ingest endpoint.",
        )

    start_time = time.time()

    # === STAGE 1: ENHANCED QUESTION CLASSIFICATION ===
    stage1_start = time.time()
    question_type, is_existence, is_feature_inquiry = enhanced_question_classifier(
        request.question.lower()
    )
    stage1_time = int((time.time() - stage1_start) * 1000)

    # === STAGE 2: INITIAL SEARCH WITH CATEGORY BOOSTING ===
    stage2_start = time.time()
    initial_results = simple_search(
        request.question, CHUNK_STORE, top_k=10
    )  # –ë–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è reranking
    stage2_time = int((time.time() - stage2_start) * 1000)

    # === STAGE 3: SEMANTIC RERANKING WITH CUSTOM RULES ===
    stage3_start = time.time()
    reranked_results = semantic_reranker_with_rules(
        initial_results, request.question, question_type
    )
    stage3_time = int((time.time() - stage3_start) * 1000)

    # === STAGE 4: ADVANCED DEDUPLICATION ===
    stage4_start = time.time()
    combined_text = " ".join([chunk["text"] for chunk in reranked_results[:7]])  # –¢–æ–ø-7 –¥–ª—è dedup
    deduplicated_sections = advanced_deduplication(combined_text)
    search_results = reranked_results[:5]  # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–ø-5
    stage4_time = int((time.time() - stage4_start) * 1000)

    search_time = stage1_time + stage2_time + stage3_time + stage4_time

    if not search_results:
        return {
            "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.",
            "citations": [],
            "debug": {
                "trace_id": f"trace_{int(time.time())}",
                "search_time_ms": search_time,
                "generation_time_ms": 0,
                "found_chunks": 0,
            },
        }

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    gen_start = time.time()
    answer = generate_answer_with_ollama(request.question, search_results)
    gen_time = int((time.time() - gen_start) * 1000)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–∏—Ç–∞—Ç—ã
    citations = []
    for result in search_results:
        citations.append(
            {
                "source": result["source"],
                "chunk_id": result["chunk_id"],
                "relevance_score": round(result["score"], 3),
                "text_preview": result["text"][:200] + "..."
                if len(result["text"]) > 200
                else result["text"],
            }
        )

    total_time = int((time.time() - start_time) * 1000)

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_metrics = calculate_search_quality_metrics(
        search_results, request.question, question_type
    )

    return {
        "answer": answer,
        "citations": citations,
        "debug": {
            "trace_id": f"trace_{int(time.time())}",
            "search_time_ms": search_time,
            "generation_time_ms": gen_time,
            "total_time_ms": total_time,
            "found_chunks": len(search_results),
            "query_terms": len(request.question.split()),
            "quality_metrics": quality_metrics,
            "reranking_pipeline": {
                "stage1_classification": {
                    "time_ms": stage1_time,
                    "question_type": question_type,
                    "is_existence": is_existence,
                    "is_feature_inquiry": is_feature_inquiry,
                },
                "stage2_initial_search": {
                    "time_ms": stage2_time,
                    "candidates_found": len(initial_results),
                    "top_scores": [round(r["score"], 3) for r in initial_results[:3]],
                },
                "stage3_semantic_reranking": {
                    "time_ms": stage3_time,
                    "reranked_count": len(reranked_results),
                    "score_improvement": round(
                        reranked_results[0]["score"] - initial_results[0]["score"], 3
                    )
                    if reranked_results and initial_results
                    else 0,
                },
                "stage4_deduplication": {
                    "time_ms": stage4_time,
                    "unique_sections": len(deduplicated_sections),
                    "final_chunks": len(search_results),
                },
            },
        },
    }


@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""

    feedback_id = f"feedback_{int(time.time())}"

    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    feedback_data = {
        "feedback_id": feedback_id,
        "timestamp": time.time(),
        "question": request.question,
        "llm_answer": request.llm_answer,
        "rating": request.rating,
        "reason": request.reason,
        "comment": request.comment,
        "session_id": request.session_id,
        "request_id": request.request_id,
    }

    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å
    print(f"üìù –ü–æ–ª—É—á–µ–Ω —Ñ–∏–¥–±–µ–∫: {request.rating} - {request.reason}")
    print(f"   –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {request.comment}")

    return {"status": "ok", "feedback_id": feedback_id, "message": "Thank you for your feedback!"}


@app.get("/documents")
async def list_documents():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    documents = []
    for doc_id, doc_data in DOCUMENT_STORE.items():
        chunks_count = len([c for c in CHUNK_STORE.values() if c["doc_id"] == doc_id])
        documents.append(
            {
                "doc_id": doc_id,
                "doc_id_short": doc_id[:8] + "...",
                "path": doc_data["path"],
                "filename": os.path.basename(doc_data["path"]),
                "size": doc_data["size"],
                "chunks": chunks_count,
                "extension": doc_data["extension"],
            }
        )

    return {
        "total_documents": len(documents),
        "total_chunks": len(CHUNK_STORE),
        "documents": documents,
    }


@app.delete("/documents/{doc_id}")
async def delete_document(doc_id: str):
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∏–º —á–∞–Ω–∫–∏"""

    if doc_id not in DOCUMENT_STORE:
        raise HTTPException(status_code=404, detail=f"Document with ID {doc_id} not found")

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
    doc_info = DOCUMENT_STORE[doc_id]

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    chunks_to_delete = [
        chunk_id for chunk_id, chunk_data in CHUNK_STORE.items() if chunk_data["doc_id"] == doc_id
    ]

    for chunk_id in chunks_to_delete:
        del CHUNK_STORE[chunk_id]

    # –£–¥–∞–ª—è–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
    del DOCUMENT_STORE[doc_id]

    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {doc_info['path']} ({len(chunks_to_delete)} —á–∞–Ω–∫–æ–≤)")

    return {
        "status": "success",
        "message": f"Document {os.path.basename(doc_info['path'])} deleted successfully",
        "deleted_chunks": len(chunks_to_delete),
        "remaining_documents": len(DOCUMENT_STORE),
        "remaining_chunks": len(CHUNK_STORE),
    }


@app.get("/stats")
async def get_statistics():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "documents_loaded": len(DOCUMENT_STORE),
        "chunks_indexed": len(CHUNK_STORE),
        "document_details": [
            {
                "doc_id": doc_id[:8] + "...",
                "path": doc_data["path"],
                "size": doc_data["size"],
                "chunks": len([c for c in CHUNK_STORE.values() if c["doc_id"] == doc_id]),
            }
            for doc_id, doc_data in DOCUMENT_STORE.items()
        ],
    }


if __name__ == "__main__":
    import uvicorn

    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ LocalRAG")
    print("üìö –≠—Ç–æ—Ç API –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    print("üîç –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ Ollama")
    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:")
    print("  ‚Ä¢ GET /health - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
    print("  ‚Ä¢ POST /ingest - –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("  ‚Ä¢ POST /ask - –≤–æ–ø—Ä–æ—Å—ã –∫ —Å–∏—Å—Ç–µ–º–µ")
    print("  ‚Ä¢ POST /feedback - –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å")
    print("  ‚Ä¢ GET /stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã")
    print("\nüåê API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:8000")
    print("üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")

    uvicorn.run(app, host="0.0.0.0", port=8000)
